{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folders ...\n",
      "Reformatting testing dataset ...\n",
      "Reformatting localization ...\n",
      "Creating the localization folder ...\n",
      "Loading annotation ...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 17] File exists: '/Users/dipaksingh/Documents/fishDetection/Dataset/localization/ALB'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-927609e41e66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-927609e41e66>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reformatting localization ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0mreformat_localization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reorganizing dataset ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-927609e41e66>\u001b[0m in \u001b[0;36mreformat_localization\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mlocalization_image_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mannotation_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mannotation_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mannotation_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mannotation_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mannotation_width\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalization_image_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalization_image_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocalization_image_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.pyc\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcurdir\u001b[0m\u001b[0;34m:\u001b[0m           \u001b[0;31m# xxx/newdir/. exists if xxx/newdir exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremovedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 17] File exists: '/Users/dipaksingh/Documents/fishDetection/Dataset/localization/ALB'"
     ]
    }
   ],
   "source": [
    "assert True\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import json\n",
    "import pylab\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.utils.vis_utils import plot\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Dataset\n",
    "DATASET_FOLDER_PATH = os.path.join(os.path.expanduser(\"~\"), \"Documents/fishDetection/Dataset\")\n",
    "TRAIN_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, \"train\")\n",
    "TEST_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, \"test_stg1\")\n",
    "LOCALIZATION_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, \"localization\")\n",
    "ANNOTATION_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, \"annotations\")\n",
    "CLUSTERING_RESULT_FILE_PATH = os.path.join(DATASET_FOLDER_PATH, \"clustering_result.npy\")\n",
    "\n",
    "# Workspace\n",
    "WORKSPACE_FOLDER_PATH = os.path.join(\"/tmp\", os.path.basename(DATASET_FOLDER_PATH))\n",
    "CLUSTERING_FOLDER_PATH = os.path.join(WORKSPACE_FOLDER_PATH, \"clustering\")\n",
    "ACTUAL_DATASET_FOLDER_PATH = os.path.join(WORKSPACE_FOLDER_PATH, \"actual_dataset\")\n",
    "ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, \"train_original\")\n",
    "ACTUAL_VALID_ORIGINAL_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, \"valid_original\")\n",
    "ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, \"train_localization\")\n",
    "ACTUAL_VALID_LOCALIZATION_FOLDER_PATH = os.path.join(ACTUAL_DATASET_FOLDER_PATH, \"valid_localization\")\n",
    "\n",
    "# Output\n",
    "OUTPUT_FOLDER_PATH = os.path.join(DATASET_FOLDER_PATH, \"{}_output\".format(os.path.basename('__file__').split(\".\")[0]))\n",
    "VISUALIZATION_FOLDER_PATH = os.path.join(OUTPUT_FOLDER_PATH, \"Visualization\")\n",
    "OPTIMAL_WEIGHTS_FOLDER_PATH = os.path.join(OUTPUT_FOLDER_PATH, \"Optimal Weights\")\n",
    "OPTIMAL_WEIGHTS_FILE_RULE = os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, \"epoch_{epoch:03d}-loss_{loss:.5f}-val_loss_{val_loss:.5f}.h5\")\n",
    "\n",
    "# Image processing\n",
    "IMAGE_ROW_SIZE = 256\n",
    "IMAGE_COLUMN_SIZE = 256\n",
    "\n",
    "# Training and Testing procedure\n",
    "MAXIMUM_EPOCH_NUM = 1000\n",
    "PATIENCE = 100\n",
    "BATCH_SIZE = 32\n",
    "INSPECT_SIZE = 4\n",
    "\n",
    "def reformat_testing_dataset():\n",
    "    # Create a dummy folder\n",
    "    dummy_test_folder_path = os.path.join(TEST_FOLDER_PATH, \"dummy\")\n",
    "    os.makedirs(dummy_test_folder_path)\n",
    "\n",
    "    # Move files to the dummy folder if needed\n",
    "    file_path_list = glob.glob(os.path.join(TEST_FOLDER_PATH, \"*\"))\n",
    "    for file_path in file_path_list:\n",
    "        if os.path.isfile(file_path):\n",
    "            shutil.move(file_path, os.path.join(dummy_test_folder_path, os.path.basename(file_path)))\n",
    "\n",
    "def load_annotation():\n",
    "    annotation_dict = {}\n",
    "    annotation_file_path_list = glob.glob(os.path.join(ANNOTATION_FOLDER_PATH, \"*.json\"))\n",
    "    for annotation_file_path in annotation_file_path_list:\n",
    "        with open(annotation_file_path) as annotation_file:\n",
    "            annotation_file_content = json.load(annotation_file)\n",
    "            for item in annotation_file_content:\n",
    "                key = os.path.basename(item[\"filename\"])\n",
    "                if key in annotation_dict:\n",
    "                    assert False, \"Found existing key {}!!!\".format(key)\n",
    "                value = []\n",
    "                for annotation in item[\"annotations\"]:\n",
    "                    value.append(np.clip((annotation[\"x\"], annotation[\"width\"], annotation[\"y\"], annotation[\"height\"]), 0, np.inf).astype(np.int))\n",
    "                annotation_dict[key] = value\n",
    "    return annotation_dict\n",
    "\n",
    "def reformat_localization():\n",
    "    print(\"Creating the localization folder ...\")\n",
    "    os.makedirs(LOCALIZATION_FOLDER_PATH)\n",
    "\n",
    "    print(\"Loading annotation ...\")\n",
    "    annotation_dict = load_annotation()\n",
    "\n",
    "    original_image_path_list = glob.glob(os.path.join(TRAIN_FOLDER_PATH, \"*/*\"))\n",
    "    for original_image_path in original_image_path_list:\n",
    "        localization_image_path = LOCALIZATION_FOLDER_PATH + original_image_path[len(TRAIN_FOLDER_PATH):]\n",
    "        if os.path.isfile(localization_image_path):\n",
    "            continue\n",
    "\n",
    "        localization_image_content = np.zeros(imread(original_image_path).shape[:2], dtype=np.uint8)\n",
    "        for annotation_x, annotation_width, annotation_y, annotation_height in annotation_dict.get(os.path.basename(original_image_path), []):\n",
    "            localization_image_content[annotation_y:annotation_y + annotation_height, annotation_x:annotation_x + annotation_width] = 255\n",
    "\n",
    "        os.makedirs(os.path.abspath(os.path.join(localization_image_path, os.pardir)))\n",
    "        imsave(localization_image_path, localization_image_content)\n",
    "\n",
    "def perform_CV(image_path_list, resized_image_row_size=64, resized_image_column_size=64):\n",
    "    if os.path.isfile(CLUSTERING_RESULT_FILE_PATH):\n",
    "        print(\"Loading clustering result ...\")\n",
    "        image_name_to_cluster_ID_array = np.load(CLUSTERING_RESULT_FILE_PATH)\n",
    "        image_name_to_cluster_ID_dict = dict(image_name_to_cluster_ID_array)\n",
    "        cluster_ID_array = np.array([image_name_to_cluster_ID_dict[os.path.basename(image_path)] for image_path in image_path_list], dtype=np.int)\n",
    "    else:\n",
    "        print(\"Reading image content ...\")\n",
    "        image_content_array = np.array([imresize(imread(image_path), (resized_image_row_size, resized_image_column_size)) for image_path in image_path_list])\n",
    "        image_content_array = np.reshape(image_content_array, (len(image_content_array), -1))\n",
    "        image_content_array = np.array([(image_content - image_content.mean()) / image_content.std() for image_content in image_content_array], dtype=np.float32)\n",
    "\n",
    "        print(\"Apply clustering ...\")\n",
    "        cluster_ID_array = DBSCAN(eps=1.5 * resized_image_row_size * resized_image_column_size, min_samples=20, metric=\"l1\", n_jobs=-1).fit_predict(image_content_array)\n",
    "\n",
    "        print(\"Saving clustering result ...\")\n",
    "        image_name_to_cluster_ID_array = np.transpose(np.vstack(([os.path.basename(image_path) for image_path in image_path_list], cluster_ID_array)))\n",
    "        np.save(CLUSTERING_RESULT_FILE_PATH, image_name_to_cluster_ID_array)\n",
    "\n",
    "    print(\"The ID value and count are as follows:\")\n",
    "    cluster_ID_values, cluster_ID_counts = np.unique(cluster_ID_array, return_counts=True)\n",
    "    for cluster_ID_value, cluster_ID_count in zip(cluster_ID_values, cluster_ID_counts):\n",
    "        print(\"{}\\t{}\".format(cluster_ID_value, cluster_ID_count))\n",
    "\n",
    "    print(\"Visualizing clustering result ...\")\n",
    "    shutil.rmtree(CLUSTERING_FOLDER_PATH, ignore_errors=True)\n",
    "    for image_path, cluster_ID in zip(image_path_list, cluster_ID_array):\n",
    "        sub_clustering_folder_path = os.path.join(CLUSTERING_FOLDER_PATH, str(cluster_ID))\n",
    "        if not os.path.isdir(sub_clustering_folder_path):\n",
    "            os.makedirs(sub_clustering_folder_path)\n",
    "        os.symlink(image_path, os.path.join(sub_clustering_folder_path, os.path.basename(image_path)))\n",
    "\n",
    "    cv_object = GroupShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "    for cv_index, (train_index_array, valid_index_array) in enumerate(cv_object.split(X=np.zeros((len(cluster_ID_array), 1)), groups=cluster_ID_array), start=1):\n",
    "        print(\"Checking cv {} ...\".format(cv_index))\n",
    "        valid_sample_ratio = len(valid_index_array) / (len(train_index_array) + len(valid_index_array))\n",
    "        if -1 in np.unique(cluster_ID_array[train_index_array]) and valid_sample_ratio > 0.15 and valid_sample_ratio < 0.25:\n",
    "            train_unique_label, train_unique_counts = np.unique([image_path.split(\"/\")[-2] for image_path in np.array(image_path_list)[train_index_array]], return_counts=True)\n",
    "            valid_unique_label, valid_unique_counts = np.unique([image_path.split(\"/\")[-2] for image_path in np.array(image_path_list)[valid_index_array]], return_counts=True)\n",
    "            if np.array_equal(train_unique_label, valid_unique_label):\n",
    "                train_unique_ratio = train_unique_counts / np.sum(train_unique_counts)\n",
    "                valid_unique_ratio = valid_unique_counts / np.sum(valid_unique_counts)\n",
    "                print(\"Using {:.2f}% original training samples as validation samples ...\".format(valid_sample_ratio * 100))\n",
    "                print(\"For training samples: {}\".format(train_unique_ratio))\n",
    "                print(\"For validation samples: {}\".format(valid_unique_ratio))\n",
    "                return train_index_array, valid_index_array\n",
    "\n",
    "    assert False\n",
    "\n",
    "def reorganize_dataset():\n",
    "    # Get list of files\n",
    "    original_image_path_list = sorted(glob.glob(os.path.join(TRAIN_FOLDER_PATH, \"*/*\")))\n",
    "    localization_image_path_list = sorted(glob.glob(os.path.join(LOCALIZATION_FOLDER_PATH, \"*/*\")))\n",
    "\n",
    "    # Sanity check\n",
    "    original_image_name_list = [os.path.basename(image_path) for image_path in original_image_path_list]\n",
    "    localization_image_name_list = [os.path.basename(image_path) for image_path in localization_image_path_list]\n",
    "    assert np.array_equal(original_image_name_list, localization_image_name_list)\n",
    "\n",
    "    # Perform Cross Validation\n",
    "    train_index_array, valid_index_array = perform_CV(original_image_path_list)\n",
    "\n",
    "    # Create symbolic links\n",
    "    shutil.rmtree(ACTUAL_DATASET_FOLDER_PATH, ignore_errors=True)\n",
    "    for (actual_original_folder_path, actual_localization_folder_path), index_array in zip(\n",
    "            ((ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH),\n",
    "            (ACTUAL_VALID_ORIGINAL_FOLDER_PATH, ACTUAL_VALID_LOCALIZATION_FOLDER_PATH)),\n",
    "            (train_index_array, valid_index_array)):\n",
    "        for index_value in index_array:\n",
    "            original_image_path = original_image_path_list[index_value]\n",
    "            localization_image_path = localization_image_path_list[index_value]\n",
    "\n",
    "            path_suffix = original_image_path[len(TRAIN_FOLDER_PATH):]\n",
    "            assert path_suffix == localization_image_path[len(LOCALIZATION_FOLDER_PATH):]\n",
    "\n",
    "            if path_suffix[1:].startswith(\"NoF\"):\n",
    "                continue\n",
    "\n",
    "            actual_original_image_path = actual_original_folder_path + path_suffix\n",
    "            actual_localization_image_path = actual_localization_folder_path + path_suffix\n",
    "\n",
    "            os.makedirs(os.path.abspath(os.path.join(actual_original_image_path, os.pardir)), exist_ok=True)\n",
    "            os.makedirs(os.path.abspath(os.path.join(actual_localization_image_path, os.pardir)), exist_ok=True)\n",
    "\n",
    "            os.symlink(original_image_path, actual_original_image_path)\n",
    "            os.symlink(localization_image_path, actual_localization_image_path)\n",
    "\n",
    "    return len(glob.glob(os.path.join(ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, \"*/*\"))), len(glob.glob(os.path.join(ACTUAL_VALID_ORIGINAL_FOLDER_PATH, \"*/*\")))\n",
    "\n",
    "def init_model(target_num=4, FC_block_num=2, FC_feature_dim=512, dropout_ratio=0.5, learning_rate=0.0001):\n",
    "    # Get the input tensor\n",
    "    input_tensor = Input(shape=(3, IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE))\n",
    "\n",
    "    # Convolutional blocks\n",
    "    pretrained_model = VGG16(include_top=False, weights=\"imagenet\")\n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable = False\n",
    "    output_tensor = pretrained_model(input_tensor)\n",
    "\n",
    "    # FullyConnected blocks\n",
    "    output_tensor = Flatten()(output_tensor)\n",
    "    for _ in range(FC_block_num):\n",
    "        output_tensor = Dense(FC_feature_dim, activation=\"relu\")(output_tensor)\n",
    "        output_tensor = BatchNormalization()(output_tensor)\n",
    "        output_tensor = Dropout(dropout_ratio)(output_tensor)\n",
    "    output_tensor = Dense(target_num, activation=\"sigmoid\")(output_tensor)\n",
    "\n",
    "    # Define and compile the model\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss=\"mse\")\n",
    "    #plot(model, to_file=os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, \"model.png\"), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def convert_localization_to_annotation(localization_array, row_size=IMAGE_ROW_SIZE, column_size=IMAGE_COLUMN_SIZE):\n",
    "    annotation_list = []\n",
    "    for localization in localization_array:\n",
    "        localization = localization[0]\n",
    "\n",
    "        mask_along_row = np.max(localization, axis=1) > 0.5\n",
    "        row_start_index = np.argmax(mask_along_row)\n",
    "        row_end_index = len(mask_along_row) - np.argmax(np.flipud(mask_along_row)) - 1\n",
    "\n",
    "        mask_along_column = np.max(localization, axis=0) > 0.5\n",
    "        column_start_index = np.argmax(mask_along_column)\n",
    "        column_end_index = len(mask_along_column) - np.argmax(np.flipud(mask_along_column)) - 1\n",
    "\n",
    "        annotation = (row_start_index / row_size, (row_end_index - row_start_index) / row_size, column_start_index / column_size, (column_end_index - column_start_index) / column_size)\n",
    "        annotation_list.append(annotation)\n",
    "\n",
    "    return np.array(annotation_list).astype(np.float32)\n",
    "\n",
    "def convert_annotation_to_localization(annotation_array, row_size=IMAGE_ROW_SIZE, column_size=IMAGE_COLUMN_SIZE):\n",
    "    localization_list = []\n",
    "    for annotation in annotation_array:\n",
    "        localization = np.zeros((row_size, column_size))\n",
    "\n",
    "        row_start_index = np.max((0, int(annotation[0] * row_size)))\n",
    "        row_end_index = np.min((row_start_index + int(annotation[1] * row_size), row_size - 1))\n",
    "\n",
    "        column_start_index = np.max((0, int(annotation[2] * column_size)))\n",
    "        column_end_index = np.min((column_start_index + int(annotation[3] * column_size), column_size - 1))\n",
    "\n",
    "        localization[row_start_index:row_end_index + 1, column_start_index:column_end_index + 1] = 1\n",
    "        localization_list.append(np.expand_dims(localization, axis=0))\n",
    "\n",
    "    return np.array(localization_list).astype(np.float32)\n",
    "\n",
    "def load_dataset(folder_path_list, color_mode_list, batch_size, classes=None, class_mode=None, shuffle=True, seed=None, apply_conversion=False):\n",
    "    # Get the generator of the dataset\n",
    "    data_generator_list = []\n",
    "    for folder_path, color_mode in zip(folder_path_list, color_mode_list):\n",
    "        data_generator_object = ImageDataGenerator(\n",
    "            rotation_range=10,\n",
    "            width_shift_range=0.05,\n",
    "            height_shift_range=0.05,\n",
    "            shear_range=0.05,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            rescale=1.0 / 255)\n",
    "        data_generator = data_generator_object.flow_from_directory(\n",
    "            directory=folder_path,\n",
    "            target_size=(IMAGE_ROW_SIZE, IMAGE_COLUMN_SIZE),\n",
    "            color_mode=color_mode,\n",
    "            classes=classes,\n",
    "            class_mode=class_mode,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed)\n",
    "        data_generator_list.append(data_generator)\n",
    "\n",
    "    # Sanity check\n",
    "    filenames_list = [data_generator.filenames for data_generator in data_generator_list]\n",
    "    assert all(filenames == filenames_list[0] for filenames in filenames_list)\n",
    "\n",
    "    if apply_conversion:\n",
    "        assert len(data_generator_list) == 2\n",
    "        for X_array, Y_array in zip(*data_generator_list):\n",
    "            yield (X_array, convert_localization_to_annotation(Y_array))\n",
    "    else:\n",
    "        for array_tuple in zip(*data_generator_list):\n",
    "            yield array_tuple\n",
    "\n",
    "class InspectPrediction(Callback):\n",
    "    def __init__(self, data_generator_list):\n",
    "        super(InspectPrediction, self).__init__()\n",
    "\n",
    "        self.data_generator_list = data_generator_list\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for data_generator_index, data_generator in enumerate(self.data_generator_list, start=1):\n",
    "            X_array, GT_Y_array = next(data_generator)\n",
    "            P_Y_array = convert_annotation_to_localization(self.model.predict_on_batch(X_array))\n",
    "\n",
    "            for sample_index, (X, GT_Y, P_Y) in enumerate(zip(X_array, GT_Y_array, P_Y_array), start=1):\n",
    "                pylab.figure()\n",
    "                pylab.subplot(1, 3, 1)\n",
    "                pylab.imshow(np.rollaxis(X, 0, 3))\n",
    "                pylab.title(\"X\")\n",
    "                pylab.axis(\"off\")\n",
    "                pylab.subplot(1, 3, 2)\n",
    "                pylab.imshow(GT_Y[0], cmap=\"gray\")\n",
    "                pylab.title(\"GT_Y\")\n",
    "                pylab.axis(\"off\")\n",
    "                pylab.subplot(1, 3, 3)\n",
    "                pylab.imshow(P_Y[0], cmap=\"gray\")\n",
    "                pylab.title(\"P_Y\")\n",
    "                pylab.axis(\"off\")\n",
    "                pylab.savefig(os.path.join(VISUALIZATION_FOLDER_PATH, \"Epoch_{}_Split_{}_Sample_{}.png\".format(epoch + 1, data_generator_index, sample_index)))\n",
    "                pylab.close()\n",
    "\n",
    "class InspectLoss(Callback):\n",
    "    def __init__(self):\n",
    "        super(InspectLoss, self).__init__()\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.valid_loss_list = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_loss = logs.get(\"loss\")\n",
    "        valid_loss = logs.get(\"val_loss\")\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.valid_loss_list.append(valid_loss)\n",
    "        epoch_index_array = np.arange(len(self.train_loss_list)) + 1\n",
    "\n",
    "        pylab.figure()\n",
    "        pylab.plot(epoch_index_array, self.train_loss_list, \"yellowgreen\", label=\"train_loss\")\n",
    "        pylab.plot(epoch_index_array, self.valid_loss_list, \"lightskyblue\", label=\"valid_loss\")\n",
    "        pylab.grid()\n",
    "        pylab.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=2, ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "        pylab.savefig(os.path.join(OUTPUT_FOLDER_PATH, \"Loss Curve.png\"))\n",
    "        pylab.close()\n",
    "\n",
    "def run():\n",
    "    print(\"Creating folders ...\")\n",
    "    os.makedirs(VISUALIZATION_FOLDER_PATH)\n",
    "    os.makedirs(OPTIMAL_WEIGHTS_FOLDER_PATH)\n",
    "\n",
    "    print(\"Reformatting testing dataset ...\")\n",
    "    reformat_testing_dataset()\n",
    "\n",
    "    print(\"Reformatting localization ...\")\n",
    "    reformat_localization()\n",
    "\n",
    "    print(\"Reorganizing dataset ...\")\n",
    "    train_sample_num, valid_sample_num = reorganize_dataset()\n",
    "\n",
    "    print(\"Initializing model ...\")\n",
    "    model = init_model()\n",
    "\n",
    "    weights_file_path_list = sorted(glob.glob(os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, \"*.h5\")))\n",
    "    if len(weights_file_path_list) == 0:\n",
    "        print(\"Performing the training procedure ...\")\n",
    "        train_generator = load_dataset(folder_path_list=[ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH], color_mode_list=[\"rgb\", \"grayscale\"], batch_size=BATCH_SIZE, seed=0, apply_conversion=True)\n",
    "        valid_generator = load_dataset(folder_path_list=[ACTUAL_VALID_ORIGINAL_FOLDER_PATH, ACTUAL_VALID_LOCALIZATION_FOLDER_PATH], color_mode_list=[\"rgb\", \"grayscale\"], batch_size=BATCH_SIZE, seed=0, apply_conversion=True)\n",
    "        train_generator_for_inspection = load_dataset(folder_path_list=[ACTUAL_TRAIN_ORIGINAL_FOLDER_PATH, ACTUAL_TRAIN_LOCALIZATION_FOLDER_PATH], color_mode_list=[\"rgb\", \"grayscale\"], batch_size=INSPECT_SIZE, seed=1)\n",
    "        valid_generator_for_inspection = load_dataset(folder_path_list=[ACTUAL_VALID_ORIGINAL_FOLDER_PATH, ACTUAL_VALID_LOCALIZATION_FOLDER_PATH], color_mode_list=[\"rgb\", \"grayscale\"], batch_size=INSPECT_SIZE, seed=1)\n",
    "        earlystopping_callback = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE)\n",
    "        modelcheckpoint_callback = ModelCheckpoint(OPTIMAL_WEIGHTS_FILE_RULE, monitor=\"val_loss\", save_best_only=True, save_weights_only=True)\n",
    "        inspectprediction_callback = InspectPrediction([train_generator_for_inspection, valid_generator_for_inspection])\n",
    "        inspectloss_callback = InspectLoss()\n",
    "        model.fit_generator(generator=train_generator,\n",
    "                            samples_per_epoch=train_sample_num,\n",
    "                            validation_data=valid_generator,\n",
    "                            nb_val_samples=valid_sample_num,\n",
    "                            callbacks=[earlystopping_callback, modelcheckpoint_callback, inspectprediction_callback, inspectloss_callback],\n",
    "                            nb_epoch=MAXIMUM_EPOCH_NUM, verbose=2)\n",
    "        weights_file_path_list = sorted(glob.glob(os.path.join(OPTIMAL_WEIGHTS_FOLDER_PATH, \"*.h5\")))\n",
    "\n",
    "    print(\"All done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
